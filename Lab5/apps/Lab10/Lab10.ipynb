{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8765e6f-17c5-43f4-9094-52f9f225538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/25 08:24:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cddbe7311ef5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark MLlib</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Spark MLlib>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Spark MLlib\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ac216d-9013-4fbe-ab7a-6bc5a99e5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|   0.5|\n",
      "|   0.1|\n",
      "|   0.6|\n",
      "|   0.9|\n",
      "|   0.0|\n",
      "|   0.7|\n",
      "|   1.2|\n",
      "+------+\n",
      "\n",
      "inputCol: input column name. (current: values)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: Binarizer_8602502eb794__output, current: features)\n",
      "outputCols: output column names. (undefined)\n",
      "threshold: Param for threshold used to binarize continuous features. The features greater than the threshold will be binarized to 1.0. The features equal to or less than the threshold will be binarized to 0.0 (default: 0.0, current: 0.5)\n",
      "thresholds: Param for array of threshold used to binarize continuous features. This is for multiple columns input. If transforming multiple columns and thresholds is not set, but threshold is set, then threshold will be applied across all columns. (undefined)\n",
      "+------+--------+\n",
      "|values|features|\n",
      "+------+--------+\n",
      "|   0.5|     0.0|\n",
      "|   0.1|     0.0|\n",
      "|   0.6|     1.0|\n",
      "|   0.9|     1.0|\n",
      "|   0.0|     0.0|\n",
      "|   0.7|     1.0|\n",
      "|   1.2|     1.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "df = spark.createDataFrame([(0.5,), (0.1,), (0.6,), (0.9,), (0.0,), (0.7,), (1.2,)], [\"values\"])\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"values\", outputCol=\"features\")\n",
    "\n",
    "df.show()\n",
    "print(binarizer.explainParams())\n",
    "isinstance(binarizer, Transformer)\n",
    "\n",
    "# i ostatcznie wykonanie transformacji\n",
    "df_transformed = binarizer.transform(df)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f9f3e6-6b33-4767-a991-e9856b76542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|zdania                                      |\n",
      "+--------------------------------------------+\n",
      "|Ala ma kota.                                |\n",
      "|Polacy nie gęsi i swój język mają.          |\n",
      "|Co ma być to będzie...                      |\n",
      "|Pan Tadeusz, czyli ostatni zajazd na Litwie.|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    ('Ala ma kota.',), \n",
    "    ('Polacy nie gęsi i swój język mają.',), \n",
    "    ('Co ma być to będzie...',), \n",
    "    ('Pan Tadeusz, czyli ostatni zajazd na Litwie.',)], \n",
    "    ['zdania'])\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8321e28d-2017-462b-9500-4a98d55aaa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------------------------------------------------+\n",
      "|zdania                                      |tokeny                                              |\n",
      "+--------------------------------------------+----------------------------------------------------+\n",
      "|Ala ma kota.                                |[ala, ma, kota.]                                    |\n",
      "|Polacy nie gęsi i swój język mają.          |[polacy, nie, gęsi, i, swój, język, mają.]          |\n",
      "|Co ma być to będzie...                      |[co, ma, być, to, będzie...]                        |\n",
      "|Pan Tadeusz, czyli ostatni zajazd na Litwie.|[pan, tadeusz,, czyli, ostatni, zajazd, na, litwie.]|\n",
      "+--------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer dzieli łancuch znaków na tokeny na podstawie białych znaków (ang. whitespace)\n",
    "tokenizer = Tokenizer(inputCol='zdania', outputCol='tokeny')\n",
    "df1_transformed = tokenizer.transform(df1)\n",
    "df1_transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76116b35-7b8a-4593-9aff-41c581c6e5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-25 08:26:32--  https://raw.githubusercontent.com/logpai/loghub/refs/heads/master/Apache/Apache_2k.log\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "connected. to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... \n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 171239 (167K) [text/plain]\n",
      "Saving to: ‘Apache_2k.log’\n",
      "\n",
      "Apache_2k.log       100%[===================>] 167.23K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-12-25 08:26:33 (3.20 MB/s) - ‘Apache_2k.log’ saved [171239/171239]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobieramy przykładowy plik\n",
    "!wget https://raw.githubusercontent.com/logpai/loghub/refs/heads/master/Apache/Apache_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ccdf41-29df-4be9-9102-724c7e62151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sun Dec 04 04:47:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties\n",
      "[Sun Dec 04 04:47:44 2005] [error] mod_jk child workerEnv in error state 6\n",
      "[Sun Dec 04 04:51:08 2005] [notice] jk2_init() Found child 6725 in scoreboard slot 10\n",
      "[Sun Dec 04 04:51:09 2005] [notice] jk2_init() Found child 6726 in scoreboard slot 8\n",
      "[Sun Dec 04 04:51:09 2005] [notice] jk2_init() Found child 6728 in scoreboard slot 6\n"
     ]
    }
   ],
   "source": [
    "!head -5 Apache_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b55a3cb3-ab3b-41a6-895a-7a4704eb8e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "|[sun dec 04 04:47:44 2005, notice, workerenv.init() ok /etc/httpd/conf/workers2.properties]|\n",
      "|[sun dec 04 04:47:44 2005, error, mod_jk child workerenv in error state 6]                 |\n",
      "|[sun dec 04 04:51:08 2005, notice, jk2_init() found child 6725 in scoreboard slot 10]      |\n",
      "|[sun dec 04 04:51:09 2005, notice, jk2_init() found child 6726 in scoreboard slot 8]       |\n",
      "|[sun dec 04 04:51:09 2005, notice, jk2_init() found child 6728 in scoreboard slot 6]       |\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.text('Apache_2k.log')\n",
    "reg_tokenizer = RegexTokenizer(pattern=']\\s\\[|]\\s|^\\[' ,inputCol='value', outputCol='tokens')\n",
    "df_transformed = reg_tokenizer.transform(df2)\n",
    "df_transformed.select(df_transformed.tokens).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ea8ce3-4e28-4129-9fb3-e187307f25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "class CustomBinarizer(\n",
    "    Transformer,\n",
    "    HasInputCol,               # Sets up an inputCol parameter\n",
    "    HasOutputCol,              # Sets up an outputCol parameter\n",
    "    DefaultParamsReadable,     # Makes parameters readable from file\n",
    "    DefaultParamsWritable      # Makes parameters writable from file\n",
    "                     ):\n",
    "\n",
    "    true_val = Param(\n",
    "        Params._dummy(),\n",
    "        \"true_val\",\n",
    "        \"List of values to be mapped as logical 1.\",\n",
    "        typeConverter=TypeConverters.toList, \n",
    "    )\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self,  inputCol=None, outputCol=None, true_val: list =None):\n",
    "        super().__init__()\n",
    "        self._setDefault(true_val=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, true_val=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "  \n",
    "    def setTrueVal(self, new_true_val):\n",
    "        return self.setParams(true_val=new_true_val)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "  \n",
    "    def getTrueVal(self):\n",
    "        return self.getOrDefault(self.true_val)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        \"\"\"\n",
    "        Przetwarza kolumnę ustawiając wartość na 1 jeżeli zawiera ona jedną z wartości true_val,\n",
    "        w przeciwnym wypadku przypisuje wartość 0\n",
    "        \"\"\"\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\n",
    "                \"No input column set for the \"\n",
    "                \"CustomBinarizer transformer.\"\n",
    "            )\n",
    "        if not self.isSet(\"true_val\"):\n",
    "            raise ValueError(\n",
    "                \"You must provide list of values to map as logical True.\"\n",
    "            )\n",
    "\n",
    "        def binarize(val):\n",
    "            \"\"\"Funkcja pomocnicza\"\"\"\n",
    "            if str(val) in self.getTrueVal():\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        binarize_udf = udf(binarize, StringType())\n",
    "        return df.withColumn(self.getOutputCol(), binarize_udf(df[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fbe28d6-46ff-4866-822d-36cf034b2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|log    |\n",
      "+-------+\n",
      "|error  |\n",
      "|warning|\n",
      "|notice |\n",
      "|info   |\n",
      "|error  |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame([('error',), ('warning',), ('notice',), ('info',), ('error',)], ['log'])\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549a9aa2-9207-471c-9755-2a2010623df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|    log|important|\n",
      "+-------+---------+\n",
      "|  error|        1|\n",
      "|warning|        1|\n",
      "| notice|        0|\n",
      "|   info|        0|\n",
      "|  error|        1|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_binarizer = CustomBinarizer(true_val=['error','warning'], inputCol='log', outputCol='important')\n",
    "df3_transformed = cust_binarizer.transform(df3)\n",
    "df3_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf8617b-0298-49dc-b1aa-37da388ab4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|    log|log_idx|\n",
      "+-------+-------+\n",
      "|  error|    0.0|\n",
      "|warning|    3.0|\n",
      "| notice|    2.0|\n",
      "|   info|    1.0|\n",
      "|  error|    0.0|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+-------------+\n",
      "|    log|log_idx|   onehot_log|\n",
      "+-------+-------+-------------+\n",
      "|  error|    0.0|(3,[0],[1.0])|\n",
      "|warning|    3.0|    (3,[],[])|\n",
      "| notice|    2.0|(3,[2],[1.0])|\n",
      "|   info|    1.0|(3,[1],[1.0])|\n",
      "|  error|    0.0|(3,[0],[1.0])|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "df3 = spark.createDataFrame([('error',), ('warning',), ('notice',), ('info',), ('error',)], ['log'])\n",
    "\n",
    "# StringIndexer to estymator zamienia wartości kategoryczne na numeryczne poprzez podanie indeksu elementu z\n",
    "# utworzonej listy wartości ze wskazanej kolumny\n",
    "stringIndexer = StringIndexer(inputCol=\"log\", outputCol=\"log_idx\")\n",
    "\n",
    "# metoda fit zwraca z kolej obiekt typu transformer\n",
    "model = stringIndexer.fit(df3)\n",
    "\n",
    "\n",
    "transformed_df3 = model.transform(df3)\n",
    "transformed_df3.show()\n",
    "\n",
    "#  teraz możemy użyć kodowania one-hot\n",
    "ohe = OneHotEncoder()\n",
    "# parametry można również ustawiać przez dedykowane metody zamiast przekazywać do konstruktora\n",
    "# w formie kwargs\n",
    "ohe.setInputCols([\"log_idx\"])\n",
    "ohe.setOutputCols([\"onehot_log\"])\n",
    "\n",
    "model = ohe.fit(transformed_df3)\n",
    "transformed_df3 = model.transform(transformed_df3)\n",
    "transformed_df3.show()\n",
    "\n",
    "# typ kolumny to będzie wektor rzadki (SparseVector)\n",
    "# przykładowy wektor: (3,[0],[1.0])\n",
    "# oznacza, że wektor ma długość 3, kolejna wartość to indeksy, na których występują wartości w tym wektorze, a ostatni to faktyczne wartości w tym wektorze\n",
    "transformed_df3.dtypes\n",
    "transformed_df3.select(transformed_df3.onehot_log).head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bff3d6-2df8-4bad-8b31-e97d951bd9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-25 08:28:44--  https://raw.githubusercontent.com/shorya1996/ML_Sklearn/refs/heads/master/Multiple%20Linear%20Regression/50_Startups.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "connected. to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... \n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 2386 (2.3K) [text/plain]\n",
      "Saving to: ‘50_Startups.csv’\n",
      "\n",
      "50_Startups.csv     100%[===================>]   2.33K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2024-12-25 08:28:44 (568 KB/s) - ‘50_Startups.csv’ saved [2386/2386]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobieramy dane\n",
    "!wget https://raw.githubusercontent.com/shorya1996/ML_Sklearn/refs/heads/master/Multiple%20Linear%20Regression/50_Startups.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492e4fb6-470b-4922-9231-0ab01698d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50_Startups.csv\n"
     ]
    }
   ],
   "source": [
    "!ls | grep *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9111cd62-0d52-4047-adaa-e4dbd08c1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('50_Startups.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37eeb859-c391-464a-97d2-e79b59e11b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e4227e-1185-4c4d-b8bf-0ba533912f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: double (nullable = true)\n",
      " |-- Administration: double (nullable = true)\n",
      " |-- Marketing Spend: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d3dc88d-ed5d-4e9a-aed5-38acc61aea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: double (nullable = true)\n",
      " |-- Administration: double (nullable = true)\n",
      " |-- Marketing Spend: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      " |-- State_numeric: double (nullable = false)\n",
      " |-- State_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# zamiana wartości kategorycznych na numeryczne\n",
    "indexer = StringIndexer(inputCol='State', outputCol='State_numeric')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)\n",
    "\n",
    "# one-hot encoding\n",
    "encoder = OneHotEncoder(inputCols=['State_numeric'], outputCols=['State_onehot'])\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36da331e-068d-4dee-b6b8-6f595e6471d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|State_numeric| State_onehot|col_onehot|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# ta funkcja zamienia wektor rzadki w tablicę\n",
    "df_col_onehot = df_onehot.select('*', vector_to_array('state_onehot').alias('col_onehot'))\n",
    "df_col_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7612aad3-3058-480e-8d73-2e41ad437d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+----------+--------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|State_numeric| State_onehot|col_onehot|California|New York|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+----------+--------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rozbijamy wartości z kolumny onehot na tzw. dummy data czyli nowe kolumny dla każdej cechy z wartością = 1\n",
    "# jeżeli dana cecha w tym wektorze występuje, 0 w przeciwnym wypadku\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "num_categories = len(df_col_onehot.first()['col_onehot']) \n",
    "cols_expanded = [(f.col('col_onehot')[i].alias(f'{indexer_fitted.labels[i]}')) for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('*', *cols_expanded)\n",
    "df_cols_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d735fbc-6ee4-4f01-8a41-18a39caa9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dobieramy tylko wybrane kolumny do ostatecznej ramki danych\n",
    "df_final = df_cols_onehot.select(\"R&D Spend\", \"Administration\", \"Marketing Spend\", \"California\", \"New York\", \"profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5785188c-7a1a-48f6-b0c2-8a0b0bf5908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+--------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|California|New York|   profit|\n",
      "+---------+--------------+---------------+----------+--------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|       0.0|     1.0|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|       1.0|     0.0|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|       0.0|     0.0|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|       0.0|     1.0|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|       0.0|     0.0|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|       0.0|     1.0|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|       1.0|     0.0|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|       0.0|     0.0| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|       0.0|     1.0|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|       1.0|     0.0|149759.96|\n",
      "+---------+--------------+---------------+----------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33cd5ea4-62c1-4e43-a45d-8bb0d5b859f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|   profit|\n",
      "+--------------------+---------+\n",
      "|[165349.2,136897....|192261.83|\n",
      "|[162597.7,151377....|191792.06|\n",
      "|[153441.51,101145...|191050.39|\n",
      "|[144372.41,118671...|182901.99|\n",
      "|[142107.34,91391....|166187.94|\n",
      "|[131876.9,99814.7...|156991.12|\n",
      "|[134615.46,147198...|156122.51|\n",
      "|[130298.13,145530...| 155752.6|\n",
      "|[120542.52,148718...|152211.77|\n",
      "|[123334.88,108679...|149759.96|\n",
      "|[101913.08,110594...|146121.95|\n",
      "|[100671.96,91790....| 144259.4|\n",
      "|[93863.75,127320....|141585.52|\n",
      "|[91992.39,135495....|134307.35|\n",
      "|[119943.24,156547...|132602.65|\n",
      "|[114523.61,122616...|129917.04|\n",
      "|[78013.11,121597....|126992.93|\n",
      "|[94657.16,145077....|125370.37|\n",
      "|[91749.16,114175....| 124266.9|\n",
      "|[86419.7,153514.1...|122776.86|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# to transformer, który składa zadane cechy (kolumny) w jeden wektor cech\n",
    "assembler = VectorAssembler(inputCols=df_final.columns[:-1],outputCol='features')\n",
    "\n",
    "data_set = assembler.transform(df_final)\n",
    "data_set = data_set.select(['features','profit'])\n",
    "data_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70df62f-8742-427d-82a3-5369999bb48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 08:30:46 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/25 08:30:46 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/12/25 08:30:46 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 13137.552791165203\n",
      "R2: 0.9099494822056451\n",
      "MSE: 172595293.34065259\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# podział danych train test\n",
    "train_data,test_data = data_set.randomSplit([0.8,0.2])\n",
    "\n",
    "# inicjalizacja modelu regresji\n",
    "lr = LinearRegression(featuresCol=\"features\",labelCol='profit', regParam=0.1)\n",
    "lrModel = lr.fit(train_data)\n",
    "test_stats = lrModel.evaluate(test_data)\n",
    "\n",
    "# wypisanie wyników\n",
    "print(f\"RMSE: {test_stats.rootMeanSquaredError}\")\n",
    "print(f\"R2: {test_stats.r2}\")\n",
    "print(f\"MSE: {test_stats.meanSquaredError}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
