{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c3fb79-1f1e-4724-8081-665cdf6fa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47346eaa-517d-466c-a939-bb710e58224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/16 13:25:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2069962cbd74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e06f1f8-7e16-4d51-a593-86d0ba1b54ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba partycji: 2\n",
      "Pierwszy element: 0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "# lista wartości zostaje podzielona na partycje i rozproszona na wszystkie dostępne węzły\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.SparkContext.parallelize.html\n",
    "rdd = spark.sparkContext.parallelize(list(range(20)))\n",
    "\n",
    "# rdd w formie rozproszonej zostaje scalone w listę zawierającą wszystkie elementy RDD\n",
    "# np. za pomocą funkcji collect()\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.RDD.collect.html\n",
    "\n",
    "rddCollect = rdd.collect()\n",
    "display(type(rddCollect))\n",
    "print(f\"Liczba partycji: {rdd.getNumPartitions()}\")\n",
    "print(f\"Pierwszy element: {rdd.first()}\")\n",
    "print(rddCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a240464-4bc1-48ec-bc4d-ffafbcc33430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obiekt RDD może przechowywać dane z różnych źródeł, które są zgodne z systemem plików Apache Hadoop\n",
    "# np. Amazon S3, Cassandra, HDFS, HBase i inne\n",
    "\n",
    "# możemy dla uniknięcia potrzeby każdorazowego odwoływania się do kontekstu poprzez spark.sparkContext zapisać sobie to w zmiennej pomocniczej\n",
    "sc = spark.sparkContext\n",
    "# tutaj wczytamy do RDD plik tekstowy\n",
    "pan_tadeusz_file = sc.textFile(\"pan-tadeusz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5455c195-d5e4-4bdf-b82f-252e089a8177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Adam Mickiewicz',\n",
       " '',\n",
       " 'Pan Tadeusz',\n",
       " 'czyli ostatni zajazd na Litwie',\n",
       " '',\n",
       " 'ISBN 978-83-288-2495-9',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(pan_tadeusz_file.getNumPartitions())\n",
    "pan_tadeusz_file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e53fac-79e8-4199-bdb8-a18f71b06a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# możemy zmienić liczbę automatycznie stworzonych partycji i ponownie rozproszyć je po węzłach\n",
    "pan_tadeusz_file = pan_tadeusz_file.repartition(4)\n",
    "pan_tadeusz_file.getNumPartitions()\n",
    "\n",
    "# również metoda coalesce może posłużyć nam do zmiany ilości partycji dla obiektu RDD np. po zastosowaniu filtrowania, które\n",
    "# znacznie zmniejsza wielkość pierwotnego obiektu RDD a co za tym idzie każdej partycji i dalsze obliczenia mogą nie być\n",
    "# wykonywane zbyt efektywnie (zbyt mały rozmiar partycji)\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.RDD.coalesce.html\n",
    "# główna różnica między repartition a coalesce jest taka, że ta pierwsza wykorzystuje mechanizm tasowania danych a ta druga może, ale nie\n",
    "# musi go wykorzystywać gdyż możemy tym sterować za pomocą parametru wywołania tej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb62077-469d-4c53-b9d4-cb2c81f69e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17366e 3:>                                                          (0 + 2) / 2]\n",
      "17129\n",
      "17307\n",
      "17293\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# jedną z funkcji dostępnej w tym API jest możliwość wykonania funkcji na każdej z partycji\n",
    "# minusem może być to, że funkcja foreachPartition zwraca typ None, więc wyniki należy przetworzyć w inny sposób\n",
    "\n",
    "def count_words(iterator):\n",
    "    words = sum([len(x.split()) for x in iterator])\n",
    "    print(words)\n",
    "\n",
    "pan_tadeusz_file.foreachPartition(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0862535-dde8-48bd-8a7c-4f427f631ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69095"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funkcje map oraz reduce\n",
    "\n",
    "# możemy również wykonać operację w inny sposób, tym raze mapując funkcję na każdy element obiektu RDD\n",
    "# zwrócony zostanie obiekt RDD, na którym możemy wykonać kolejne operacje\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).take(10)\n",
    "\n",
    "# np. reduce - i tu nawiązanie do dość znanej techniki przetwarzania, MapReduce\n",
    "# więcej: https://en.wikipedia.org/wiki/MapReduce\n",
    "# oraz: https://wiadrodanych.pl/big-data/jak-dziala-mapreduce/\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d01c105-9eca-4713-8cbf-1c386a9665b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69095"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lub tak - ten sam efekt\n",
    "from operator import add\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1d0b94-404d-47f6-99bf-9f95ae372de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'zaraz', 'mogłem', 'pieszo,', 'do', 'Twych', 'świątyń', 'progu'],\n",
       " ['Iść', 'za', 'wrócone', 'życie', 'podziękować', 'Bogu),'],\n",
       " ['Tak', 'nas', 'powrócisz', 'cudem', 'na', 'Ojczyzny', 'łono.'],\n",
       " ['Tymczasem', 'przenoś', 'moją', 'duszę', 'utęsknioną'],\n",
       " ['Do', 'tych', 'pagórków', 'leśnych,', 'do', 'tych', 'łąk', 'zielonych,'],\n",
       " ['Szeroko', 'nad', 'błękitnym', 'Niemnem', 'rozciągnionych;'],\n",
       " ['Do', 'tych', 'pól', 'malowanych', 'zbożem', 'rozmaitem,'],\n",
       " ['Wyzłacanych', 'pszenicą,', 'posrebrzanych', 'żytem;'],\n",
       " ['Gdzie', 'bursztynowy', 'świerzop,', 'gryka', 'jak', 'śnieg', 'biała,'],\n",
       " ['Gdzie', 'panieńskim', 'rumieńcem', 'dzięcielina', 'pała,']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'zaraz',\n",
       " 'mogłem',\n",
       " 'pieszo,',\n",
       " 'do',\n",
       " 'Twych',\n",
       " 'świątyń',\n",
       " 'progu',\n",
       " 'Iść',\n",
       " 'za']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# różnica między map() a flatMap() dla tego przypadku\n",
    "display(pan_tadeusz_file.map(lambda s: s.split()).take(10))\n",
    "pan_tadeusz_file.flatMap(lambda s: s.split()).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078d060-7c2c-4c13-a2e8-d7edb7c69d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
