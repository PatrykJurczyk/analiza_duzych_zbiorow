{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89eef3e2-ee6b-47a4-abd0-1cbcd4573c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff4bd92-e23c-4779-aa61-6e3b762bf517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/16 18:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/16 18:30:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2069962cbd74:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache SQL and Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Apache SQL and Hive>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ścieżka do bazy danych hurtowni danych oraz plików\n",
    "# należy dostosować do ścieżki względnej, w której umieszczony został bieżący notebook\n",
    "warehouse_location = '/opt/spark/work-dir/Lab7/metastore_db'\n",
    "\n",
    "# utworzenie sesji Spark, ze wskazaniem włączenia obsługi Hive oraz\n",
    "# lokalizacją przechowywania hurtowni danych\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Apache SQL and Hive\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e491acac-45e8-4712-8eb3-7ba453d98822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# dostosuj ścieżkę do pliku do swoich danych, tutaj został utworzony mniejszy zbiór niż w poprzednim labie\n",
    "df = spark.read.csv('Lab7/employee_1m.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62639acd-0271-4d92-bedd-30dda2e26a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy widok tymczasowy w pamięci węzła\n",
    "df.createOrReplaceTempView(\"EMPLOYEE_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59d51b2-d817-407a-aff3-aa50e598bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 18:30:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/12/16 18:30:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/12/16 18:30:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/12/16 18:30:51 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "24/12/16 18:30:51 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "24/12/16 18:30:51 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli, zwróć uwagę na to, czy stworzona tabela jest tymczasowa czy trwała\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55571f3-3fbc-4000-afa5-3eff165868b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---+-------+\n",
      "| id| firstname|  lastname|age| salary|\n",
      "+---+----------+----------+---+-------+\n",
      "|  1|Mieczysław|    Wlotka| 21| 8653.2|\n",
      "|  2|     Marek|  Kowalski| 23|5973.43|\n",
      "|  3|   Wisława|     Pysla| 38|9544.06|\n",
      "|  4|Aleksandra|Malinowski| 62|8804.64|\n",
      "+---+----------+----------+---+-------+\n",
      "\n",
      "+----------+\n",
      "| firstname|\n",
      "+----------+\n",
      "|Mieczysław|\n",
      "|     Marek|\n",
      "|   Wisława|\n",
      "|Aleksandra|\n",
      "| Krzysztof|\n",
      "|  Wojciech|\n",
      "|     Agata|\n",
      "|Aleksandra|\n",
      "|      Adam|\n",
      "|     Marek|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych jak z tabeli SQL\n",
    "spark.sql(\"Select * from EMPLOYEE_DATA limit 4\").show()\n",
    "spark.sql(\"select firstname from EMPLOYEE_DATA\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a90f0b-d06e-44f8-af9c-c47bbd145ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+\n",
      "| firstname|count(firstname)|       avg(salary)|\n",
      "+----------+----------------+------------------+\n",
      "|   Wisława|           99846| 7851.188853233986|\n",
      "|Mieczysław|           99636| 7848.163473242557|\n",
      "|     Agata|          100255|  7849.10577158249|\n",
      "| Krzysztof|           99593| 7851.736728886598|\n",
      "|     Marek|          100260|7851.2525260323155|\n",
      "|      Adam|           99888|7849.1089016698925|\n",
      "| Katarzyna|           99902|  7847.58591829992|\n",
      "|  Wojciech|          100379|7851.9279910140585|\n",
      "|  Zbigniew|          100075| 7845.564227529331|\n",
      "|Aleksandra|          100166|  7848.51743855202|\n",
      "+----------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select firstname, count(firstname), avg(salary) from EMPLOYEE_DATA group by firstname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fe63e6-de5f-4409-8fa6-30b562369aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------------+\n",
      "| firstname|  lastname| salary|after_rising|\n",
      "+----------+----------+-------+------------+\n",
      "|Mieczysław|    Wlotka| 8653.2|     9518.52|\n",
      "|     Marek|  Kowalski|5973.43|     6570.77|\n",
      "|   Wisława|     Pysla|9544.06|    10498.47|\n",
      "|Aleksandra|Malinowski|8804.64|      9685.1|\n",
      "| Krzysztof|     Pysla|8252.06|     9077.27|\n",
      "+----------+----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rising = 0.1 # 10% podwyżki\n",
    "spark.sql(f\"select firstname, lastname, salary, round(salary + salary * {rising},2) as after_rising from EMPLOYEE_DATA\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8c56ae-a1a5-4396-8d11-7448b200f0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7236b311-fbe6-4155-b419-17584e4b3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla zrealizowania kolejnych przykładów dokonamy kilku modyfikacji pliku employee\n",
    "# 1. dodanie kolumny ID - indeksu\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436d07ad-c22f-477d-9f6b-ee3cf60255cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+---+-------+\n",
      "| ID| firstname|           lastname|age| salary|\n",
      "+---+----------+-------------------+---+-------+\n",
      "|  0|Mieczysław|             Wlotka| 21| 8653.2|\n",
      "|  1|     Marek|           Kowalski| 23|5973.43|\n",
      "|  2|   Wisława|              Pysla| 38|9544.06|\n",
      "|  3|Aleksandra|         Malinowski| 62|8804.64|\n",
      "|  4| Krzysztof|              Pysla| 27|8252.06|\n",
      "|  5|  Wojciech|             Wlotka| 48| 6845.1|\n",
      "|  6|     Agata|Brzęczyszczykiewicz| 32|7267.86|\n",
      "|  7|Aleksandra|           Kowalski| 40|8924.74|\n",
      "|  8|      Adam|       Mieczykowski| 40| 7417.5|\n",
      "|  9|     Marek|             Szczaw| 66|7308.44|\n",
      "+---+----------+-------------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "590d7431-9066-4075-830d-7fbdd777af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dokonamy podziału danych i zapisania w różnych formatach\n",
    "splits = df.randomSplit(weights=[0.3, 0.7], seed=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104cccbd-dcff-47b1-9097-87fb588614c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298931, 701069)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].count(), splits[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beeee723-09e9-4336-8055-622fd816a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dość dziwne zjawisko niezbyt równego podziału danych jest opisane w artykułach:\n",
    "# https://medium.com/udemy-engineering/pyspark-under-the-hood-randomsplit-and-sample-inconsistencies-examined-7c6ec62644bc\n",
    "# oraz\n",
    "# https://www.geeksforgeeks.org/pyspark-randomsplit-and-sample-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eaa952c-641b-4bbd-9a6d-6d29bc224f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# większa część trafi do nowej tymczasowej tabeli\n",
    "splits[1].createOrReplaceTempView(\"EMPLOYEE_DATA_SPLIT_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c2de80-3288-449c-8a21-1ab377ffed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# a mniejsza do plików JSON\n",
    "splits[0].write.json('employee_data.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455c9fe9-3f06-48fb-8649-75ef147857ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./employee_data.json/part-00000-0e75328b-13d9-4878-a8ef-ded6720a090a-c000.json\n",
      "./employee_data.json/part-00001-0e75328b-13d9-4878-a8ef-ded6720a090a-c000.json\n"
     ]
    }
   ],
   "source": [
    "!ls ./employee_data.json/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "471e1932-553d-4a8f-84dc-182dc57137e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aby móc wykorzystać dane w przykładach ze złączaniem, zapiszemy jeszcze próbkę danych z głównej ramki\n",
    "# z identyfikatorami oraz dodatkową kolumną z podwyżką\n",
    "from pyspark.sql.functions import col, lit, round\n",
    "\n",
    "lucky_guys = spark.sql(\"select * from EMPLOYEE_DATA\").sample(0.01)\\\n",
    ".withColumn('rising', lit('10%')).withColumn('salary after rising', round(col('salary') * 1.1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd645d01-8ad7-453b-b0c7-b4fb04f570cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 18:32:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/12/16 18:32:44 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/12/16 18:32:44 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/12/16 18:32:44 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# zapisujemy szczęściarzy do oddzielnej tabeli w hurtowni\n",
    "lucky_guys.write.mode('overwrite').saveAsTable(\"lucky_employees\", format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1891bdd-a5b2-4341-bcf8-71353891fcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfed75a8-0d7b-435f-a8da-1166d7a043e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Lab7/metastore_db/lucky_employees/part-00000-4f2163c5-a412-47fa-9bd1-2d8ad08f55f6-c000.snappy.parquet\n",
      "./Lab7/metastore_db/lucky_employees/part-00001-4f2163c5-a412-47fa-9bd1-2d8ad08f55f6-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ./Lab7/metastore_db/lucky_employees/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5881cd7-dd6a-4e19-9cd8-0e8e25afa7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 18:32:54 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException\n",
      "24/12/16 18:32:54 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------+------+-------------------+\n",
      "|  ID| firstname|  lastname| salary|rising|salary after rising|\n",
      "+----+----------+----------+-------+------+-------------------+\n",
      "| 235|   Wisława|Wróblewski|7911.75|   10%|            8702.93|\n",
      "| 346| Katarzyna|     Pysla|7008.55|   10%|            7709.41|\n",
      "| 688|Aleksandra|    Szczaw|6089.69|   10%|            6698.66|\n",
      "| 832| Krzysztof|    Szczaw|6348.54|   10%|            6983.39|\n",
      "|1206|Aleksandra|    Wlotka|7540.97|   10%|            8295.07|\n",
      "|1297|Aleksandra|  Kowalski|6756.39|   10%|            7432.03|\n",
      "|1320| Krzysztof|      Glut|6746.98|   10%|            7421.68|\n",
      "|2857|     Marek|  Barański|7624.63|   10%|            8387.09|\n",
      "|2868|      Adam|Malinowski|7646.53|   10%|            8411.18|\n",
      "|3038|  Zbigniew|    Wlotka| 7472.0|   10%|             8219.2|\n",
      "+----+----------+----------+-------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# przykład złączania danych na różnych źródłach danych\n",
    "# zapytanie SQL bezpośrednio na plikach - tutaj zapisanych wcześniej JSON-ach oraz parquet\n",
    "query = \"\"\"\n",
    "SELECT ed.ID, ed.firstname, ed.lastname, ed.salary, lucky.rising, lucky.`salary after rising`\n",
    "FROM json.`./employee_data.json/` as jtable \n",
    "join EMPLOYEE_DATA ed on jtable.ID=ed.ID \n",
    "join parquet.`./Lab7/metastore_db/lucky_employees/` as lucky on ed.ID=lucky.ID\n",
    "\"\"\"\n",
    "df_from_json = spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4e0e6b8-e952-4188-9a6d-e3da632338f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ten przykład pokazuje podział na 16 wiaderek danych bazując na podziale po kolumnie ID (tu używane jest hashowanie)\n",
    "# dane posortowane są w każdym buckecie po kolumnie salary\n",
    "# dane zapisywane są do hurtowni Hive, a informacje o zapisanych tam tabelach przechowywane są w\n",
    "# Hive metastore (domyślnie jest do baza danych Derby)\n",
    "df.write.bucketBy(16, 'ID').mode('overwrite').sortBy('salary').saveAsTable('employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07eed003-9d10-4b58-af24-554b0a234209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00000.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00001.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00002.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00003.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00004.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00005.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00006.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00007.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00008.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00009.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00010.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00011.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00012.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00013.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00014.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00000-010a5756-11d3-45fc-9e83-11d9b6219aa3_00015.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00000.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00001.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00002.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00003.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00004.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00005.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00006.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00007.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00008.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00009.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00010.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00011.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00012.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00013.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00014.c000.snappy.parquet\n",
      "Lab7/metastore_db/employee_id_bucketed/part-00001-010a5756-11d3-45fc-9e83-11d9b6219aa3_00015.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls Lab7/metastore_db/employee_id_bucketed/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f781bdb3-4913-4cc8-97b3-912d0ee3854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------+---+-------+\n",
      "|    ID| firstname|           lastname|age| salary|\n",
      "+------+----------+-------------------+---+-------+\n",
      "| 74556|  Zbigniew|           Barański| 54|3314.95|\n",
      "| 23035|  Wojciech|       Mieczykowski| 32|3781.23|\n",
      "|474578|Aleksandra|Brzęczyszczykiewicz| 41|4045.36|\n",
      "|159298|      Adam|               Glut| 38| 4091.0|\n",
      "|513360|  Wojciech|             Wlotka| 64|4111.58|\n",
      "|372482|  Wojciech|           Barański| 24|4213.19|\n",
      "|103914|  Wojciech|           Kowalski| 64|4403.96|\n",
      "|381730|Aleksandra|         Malinowski| 18|4419.66|\n",
      "|493673|     Agata|               Glut| 43| 4428.0|\n",
      "|168018|     Marek|           Barański| 39|4439.07|\n",
      "+------+----------+-------------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('employee_id_bucketed').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9e71d5-c34b-4fe5-873d-e4c0562ea9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_id_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beac145d-418e-4a30-ab55-b5e588955db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usunięcie tabeli\n",
    "spark.sql('DROP TABLE employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20331e94-e390-4ae6-a693-55eab04c65e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# jeżeli dane, z którymi pracujemy zawierają stosunkowo niewiele różnorodnych wartości w danych kolumnach\n",
    "# lub filtrowanie i obliczenia często odbywają się na podgrupach danych to lepsze efekty uzyskamy\n",
    "# poprzez wykorzystanie możliwości partycjonowania tych danych, które to partycjonowanie\n",
    "# będzie również odzwierciedlone w fizycznej strukturze plików na dysku twardym w hurtowni danych\n",
    "\n",
    "# zobaczmy przykład poniżej\n",
    "\n",
    "df.write.partitionBy(\"lastname\").mode('overwrite').saveAsTable(\"employees_partitioned_lastname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d5076c7-227c-4cff-a21a-84805b652a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dobrym pomysłem jest też określenie ilości bucketów wynikających z danych w konkretnej kolumnie\n",
    "# i wykorzystanie do podziału\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.bucketBy.html\n",
    "buckets = spark.sql(\"select distinct firstname from EMPLOYEE_DATA\").count()\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c017c46f-a8ba-4408-a2f8-c186114d8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lastname=Barański'\t\t'lastname=Malinowski'\t 'lastname=Wlotka'\n",
      "'lastname=Brzęczyszczykiewicz'\t'lastname=Mieczykowski'  'lastname=Wróblewski'\n",
      "'lastname=Glut'\t\t\t'lastname=Pysla'\t  _SUCCESS\n",
      "'lastname=Kowalski'\t\t'lastname=Szczaw'\n"
     ]
    }
   ],
   "source": [
    "# widok danych podzielonych na partycję z punktu widzenia systemu plików\n",
    "!ls Lab7/metastore_db/employees_partitioned_lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "127a7e66-4c41-4b18-8519-1a83d4b84e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#19], functions=[avg(salary#21)])\n",
      "   +- Exchange hashpartitioning(lastname#19, 200), ENSURE_REQUIREMENTS, [plan_id=693]\n",
      "      +- HashAggregate(keys=[lastname#19], functions=[partial_avg(salary#21)])\n",
      "         +- Filter (isnotnull(lastname#19) AND (lastname#19 = Pysla))\n",
      "            +- FileScan csv [lastname#19,salary#21] Batched: false, DataFilters: [isnotnull(lastname#19), (lastname#19 = Pysla)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/Lab7/employee_1m.csv], PartitionFilters: [], PushedFilters: [IsNotNull(lastname), EqualTo(lastname,Pysla)], ReadSchema: struct<lastname:string,salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e02fba24-724c-42b0-b9fe-68bc2f1211d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|lastname|       avg(salary)|\n",
      "+--------+------------------+\n",
      "|   Pysla|7847.1464690294915|\n",
      "+--------+------------------+\n",
      "\n",
      "CPU times: user 10.4 ms, sys: 0 ns, total: 10.4 ms\n",
      "Wall time: 1.11 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de421004-b730-4ded-8c47-10bc2f6d436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#507], functions=[avg(salary#506)])\n",
      "   +- Exchange hashpartitioning(lastname#507, 200), ENSURE_REQUIREMENTS, [plan_id=759]\n",
      "      +- HashAggregate(keys=[lastname#507], functions=[partial_avg(salary#506)])\n",
      "         +- FileScan parquet spark_catalog.default.employees_partitioned_lastname[salary#506,lastname#507] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/Lab7/metastore_db/employees_partitioned_lastn..., PartitionFilters: [isnotnull(lastname#507), (lastname#507 = Pysla)], PushedFilters: [], ReadSchema: struct<salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8546b7fb-6d3a-411a-8636-08773834241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|lastname|       avg(salary)|\n",
      "+--------+------------------+\n",
      "|   Pysla|7847.1464690294915|\n",
      "+--------+------------------+\n",
      "\n",
      "CPU times: user 2.95 ms, sys: 1.09 ms, total: 4.03 ms\n",
      "Wall time: 318 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a72b3d49-ba85-4187-81fb-437abd0cfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
