{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb78002-c426-4853-bf1c-1cd1e391bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639cfa34-012c-41bb-849f-3d9da553914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/16 17:50:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/16 17:50:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/16 17:50:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2069962cbd74:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"6g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5e521f-0089-4fa2-b1d3-2abb71684856",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d60986-55f1-4e77-b9f8-ff2b49988300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-16 17:50:05--  https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip\n",
      "128.195.10.252ive.ics.uci.edu (archive.ics.uci.edu)... \n",
      "connected. to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘recipe+reviews+and+user+feedback+dataset.zip’\n",
      "\n",
      "recipe+reviews+and+     [      <=>           ]   2.02M  1.15MB/s    in 1.7s    \n",
      "\n",
      "2024-12-16 17:50:08 (1.15 MB/s) - ‘recipe+reviews+and+user+feedback+dataset.zip’ saved [2114088]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobranie spakowanego zbioru za pomocą polecenia systemowego wget\n",
    "# strona datasetu: https://archive.ics.uci.edu/dataset/911/recipe+reviews+and+user+feedback+dataset\n",
    "!wget https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb871ea-7622-48e2-a08d-cfd56616d756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  Lab6.ipynb\t\t\t\t    Zad1.ipynb\n",
      "lab   recipe+reviews+and+user+feedback+dataset.zip  Zad2i3i4.ipynb\n"
     ]
    }
   ],
   "source": [
    "# listujemy zawartość bieżącego folderu\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6de679d-a4e0-4e8a-831a-a4c7fbfddf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmiana nazwy pliku - nie jest konieczna, ale trzeba zmienić później ścieżkę w kolejnej komórce notatnika\n",
    "!mv recipe+reviews+and+user+feedback+dataset.zip recipe_reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74c1052-fbce-45b9-8057-cef61061fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wypakowujemy plik do podfolderu data\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"recipe_reviews.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49196765-f879-4e71-8a6f-b159d087b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Recipe Reviews and User Feedback Dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e5b056-afed-4cdc-bb1b-f32d71fb47c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",recipe_number,recipe_code,recipe_name,comment_id,user_id,user_name,user_reputation,created_at,reply_count,thumbs_up,thumbs_down,stars,best_score,text\n",
      "0,001,14299,Creamy White Chili,sp_aUSaElGf_14299_c_2G3aneMRgRMZwXqIHmSdXSG1hEM,u_9iFLIhMa8QaG,Jeri326,1,1665619889,0,0,0,5,527,\"I tweaked it a little, removed onions because of onion haters in my house, used Italian seasoning instead of just oregano, and use a paprika/ cayenne mix and a little more than the recipe called for.. we like everything a bit more hot. The chili was amazing! It was easy to make and everyone absolutely loved it. It will now be a staple meal in our house.\"\n",
      "1,001,14299,Creamy White Chili,sp_aUSaElGf_14299_c_2FsPC83HtzCsQAtOxlbL6RcaPbY,u_Lu6p25tmE77j,Mark467,50,1665277687,0,7,0,5,724,\"Bush used to have a white chili bean and it made this recipe super simple. I’ve written to them and asked them to please!, bring them back\"\n"
     ]
    }
   ],
   "source": [
    "# sprawdzamy jak wyglądają 3 pierwsze linie pliku, widać, że pierwsza zawiera nagłówki kolumn a dane są oddzielone przecinkiem\n",
    "!head -3 \"data/Recipe Reviews and User Feedback Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace7543c-e5dc-418f-abf0-c5aa91d1c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89c9547-8120-4a51-bff3-9943287b99d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|_c0|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id| user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|   Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|   Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  3|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_fqrybAdYjgjG|jeansch123|              1|1661787808|          2|        2|          0|    0|       581|In your introduct...|\n",
      "|  4|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_XXWKwVhKZD69|  camper77|             10|1664913823|          1|        7|          0|    0|       820|Wonderful! I made...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# najpopularniejsza metoda ich pobrania to show(), ale jest ich więcej\n",
    "df_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a84be8-afca-4805-a081-c0ee369cd3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rzut oka na schemę tego DataFrame\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47086bc5-8605-45ad-993e-e142d6497dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widać, że wszystkie kolumny są typu string, to jest domyślny sposób wczytywania danych przez spark z plain text\n",
    "# możemy jednak przekazać dodatkowy parametr, który na podstawie próbki danych spróbuje dobrać typ danych odpowiedni dla kolumny\n",
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda5ffcb-5713-45dc-947f-92709651e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- thumbs_up: integer (nullable = true)\n",
      " |-- thumbs_down: integer (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- best_score: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# po wypisaniu schemy widać zmianę\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1db6334-a329-4914-81d2-e3d6b77f5825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ramkę możemy również inicjalizować wskazując pożądane typy danych\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, LongType\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",36636,\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",40288,\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",42114,\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",39192,\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",1000)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\", StringType(), True), \\\n",
    "    StructField(\"user_id\", StringType(), True), \\\n",
    "    StructField(\"lastname\", StringType(), True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    # błąd konwersji \"\" na int!\n",
    "    # StructField(\"id\", LongType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", StringType(), True)\n",
    "    # chcielibyśmy tak, ale tutaj nie da się za bardzo - błąd konwersji int na decimal!\n",
    "    # StructField(\"salary\", DecimalType(10,2), True) \\\n",
    "  ])\n",
    "\n",
    "df_test = spark.createDataFrame(data=data,schema=schema)\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea92c5a6-260c-4330-9f9d-7176d0c32670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# możemy wykonać rzutowanie po wczytaniu danych z większością kolumn typu tekstowego\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_test = df_test.withColumn(\"salary\", F.col(\"salary\").cast(\"decimal(10,2)\"))\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e935fe8-6d80-4286-90b7-dff83fb0eebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| salary|\n",
      "+-------+\n",
      "|3000.00|\n",
      "|4000.00|\n",
      "|4000.00|\n",
      "|4000.00|\n",
      "|1000.00|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# wyświetlenie danych z pojedynczej kolumny\n",
    "df_test.select(df_test.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b12c85b7-c282-4dd6-8a39-4462ec60d7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18268"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ile wierszy w ramce?\n",
    "df_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6c9ce82-e0f1-4109-98e0-56dd609636b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'user_name'>, Column<'user_name'>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame składa się z obiektów typu Column dla każdej kolumny\n",
    "# API dla typu Column: https://spark.apache.org/docs/3.5.3/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html\n",
    "\n",
    "# do kolumn możemy się odwoływać tak jak w pandas API, ale wynik jest inny\n",
    "df_reviews.user_name, df_reviews['user_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "356ad802-010b-4bb3-932b-3195dc985099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| user_name|\n",
      "+----------+\n",
      "|   Jeri326|\n",
      "|   Mark467|\n",
      "|Barbara566|\n",
      "|jeansch123|\n",
      "|  camper77|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aby wyświetlić dane musimy wywoałać funkcję select na obiekcie dataframe\n",
    "\n",
    "df_reviews.select(df_reviews.user_name).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aef684e-810f-45d3-ba3f-0bd85c3d35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[user_name: string, user_reputation: string]\n",
      "+----------+---------------+\n",
      "| user_name|user_reputation|\n",
      "+----------+---------------+\n",
      "|   Jeri326|              1|\n",
      "|   Mark467|             50|\n",
      "|Barbara566|             10|\n",
      "|jeansch123|              1|\n",
      "|  camper77|             10|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do funkcji select możemy przekazać wiele kolumn a wywołania podobnie jak dla RDD są leniwe\n",
    "print(df_reviews.select(df_reviews.user_name, df_reviews.user_reputation))\n",
    "# musimy więc wywołać funkcję, której wykonanie \"zmusi\" Sparka do wyliczenia jej wartości lub jawnie wywołać np. show\n",
    "df_reviews.select(df_reviews.user_name, df_reviews.user_reputation).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c16f2d9-d993-421a-8b49-0bf36f3778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# można zmienić to domyślne zachowanie Spark, ale zazwyczaj nie jest to dobry pomysł, chyba, że zbiór jest mały\n",
    "# zmieniamy to poprzez edycję poniższego parametru\n",
    "# spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "989a9e67-66df-4c17-862c-f1f6ff4c5656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "| user_name|user_reputation|\n",
      "+----------+---------------+\n",
      "|   Jeri326|              1|\n",
      "|   Mark467|             50|\n",
      "|Barbara566|             10|\n",
      "|jeansch123|              1|\n",
      "|  camper77|             10|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lub indeksując kolumny innym sposobem\n",
    "df_reviews.select(df_reviews['user_name'],df_reviews['user_reputation']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec5c3d23-908e-40b3-bbef-3d1d01489ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n",
      "[Stage 12:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|_c0|recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|  0|           45|         64|         74|        77|     80|       83|             84|        86|         86|       86|         86|   86|        86|  86|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# policzymy teraz liczbę wartości NULL w każdej kolumnie\n",
    "df_reviews.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_reviews.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a271b0e7-524b-4002-bee4-c3af437817fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|                 _c0|       recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|      Thank you!!!!\"|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| It was excellent!  |                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|The recipe was a ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|A shoutout to the...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|This recipe is de...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| like the time it...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|My one complaint ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|Have never eaten ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I thinned the mix...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|When you write 1-...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|decided to make a...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|looked good.  I u...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|little sweeter th...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I baked the crust...| just to crisp it...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I made it myself ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I will try adding...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|This recipe is a ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| big! I&#39;m wit...| WILL NOT MAKE it...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|It says a small p...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|                  So| I used the compl...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# rzućmy okiem na kilka wierszy gdzie w kolumnie recipe_name jest wartość NULL\n",
    "df_reviews.filter(df_reviews.recipe_code.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b964dcb6-9c89-4b01-bed6-3c657f0670c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18182"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zapisanie do nowej ramki danych bez wartości pustych\n",
    "df_reviews_clean = df_reviews.na.drop()\n",
    "df_reviews_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7beadaec-1701-4782-b527-8964eac9bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|_c0|recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|  0|            0|          0|          0|         0|      0|        0|              0|         0|          0|        0|          0|    0|         0|   0|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dla pewności możemy to sprawdzić raz jeszcze\n",
    "df_reviews_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_reviews_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76815067-6914-4325-abb7-e03438e5c354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     user_name|\n",
      "+--------------+\n",
      "|         ahmom|\n",
      "|  annamossburg|\n",
      "|   astarzynski|\n",
      "|     adamscook|\n",
      "|      angela32|\n",
      "|       annaf27|\n",
      "|    avanhaasen|\n",
      "|    aunt ann's|\n",
      "|     angelic0w|\n",
      "|a_n_g_e_l_0715|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|_c0|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id|       user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|         Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|         Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|      Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  5|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_BALTQJIvWtYr|         nikhita|              1|1661354351|          0|        3|          1|    5|       518|amazing! my boyfr...|\n",
      "|  6|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_HuJVXMzQqJoI|       Sandy1256|              1|1644088805|          0|       11|          0|    5|       833|Wow!!!  This reci...|\n",
      "|  8|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_xDTU4BqIVIc9|           Quest|              1|1643933124|          0|        6|          0|    5|       693|I absolutely love...|\n",
      "|  9|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_cDoX9ujcQEoc|     Susannah953|              1|1643237839|          0|        0|          0|    5|       404|I make this a lot...|\n",
      "| 10|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_a1kMC63ejmcn|jillanglemyer810|              1|1643127683|          0|        3|          0|    5|       706|Best and easiest ...|\n",
      "| 11|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_rLDRFmNx35zU|       Leslie126|              1|1642892566|          0|        2|          1|    5|       617|Best white chili ...|\n",
      "| 12|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_lPW6uyGJNSN0|       Cindy4876|              1|1642353692|          0|        1|          0|    5|       633|This recipe was e...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n",
      "24/12/16 17:50:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# filtrowanie danych z ramki\n",
    "df_reviews_clean.filter(df_reviews.user_name.startswith('a')).select(df_reviews_clean.user_name).show(10)\n",
    "df_reviews_clean.filter(df_reviews.stars == 5).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31605bd1-fe94-46f7-a496-6d60bc1711d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    avg(thumbs_up)|\n",
      "+------------------+\n",
      "|1.0892641073589264|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# wyliczenie średniej wartości z kolumny\n",
    "df_reviews_clean.select(avg(df_reviews_clean.thumbs_up)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28fbe3c2-2d78-4fb9-832b-d1363f40989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         thumbs_up|\n",
      "+-------+------------------+\n",
      "|  count|             18182|\n",
      "|   mean|1.0892641073589264|\n",
      "| stddev| 4.201003572820717|\n",
      "|    min|                 0|\n",
      "|    max|               106|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ale możemy się dowiedzieć tego i więcej w sposób podobny do tego z biblioteki pandas\n",
    "df_reviews_clean.select(df_reviews_clean.thumbs_up).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28691984-cb91-48e9-85dd-cd129bef53d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:50:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/Lab6/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|recipe_code|sum(thumbs_down)|\n",
      "+-----------+----------------+\n",
      "|       2832|             488|\n",
      "|       9739|             354|\n",
      "|      17826|             328|\n",
      "|      18345|             313|\n",
      "|      12003|             306|\n",
      "|       4383|             301|\n",
      "|      41095|             275|\n",
      "|       8202|             272|\n",
      "|       6504|             264|\n",
      "|       6086|             262|\n",
      "+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df_reviews_clean.groupby('recipe_code').agg({'thumbs_down': 'sum'}).sort(desc('sum(thumbs_down)')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4282fb9c-af5f-4060-bc92-b32c816f93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deklaracja zbiorów wartości dla poszczególnych kolumn przyszłego zbioru danych\n",
    "header = ['id', 'firstname', 'lastname', 'age', 'salary']\n",
    "firstnames = ['Adam', 'Katarzyna', 'Krzysztof', 'Marek', 'Aleksandra', 'Zbigniew', 'Wojciech', 'Mieczysław', 'Agata', 'Wisława']\n",
    "lastnames = ['Mieczykowski', 'Kowalski', 'Malinowski' , 'Szczaw', 'Glut', 'Barański', 'Brzęczyszczykiewicz', 'Wróblewski', 'Wlotka', 'Pysla']\n",
    "age = {'min': 18, 'max': 68}\n",
    "salary = {'min': 3200, 'max': 12500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4731743-2b70-4727-9336-3287ee07bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja do generowania fikcyjnego datasetu\n",
    "# n_rows oznacza ilość wierszy, którą chcemy finalnie uzyskać\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_dataset(filename, n_rows=100, chunk_size=100000):\n",
    "    rows = []\n",
    "    rows.append(header)\n",
    "    mu = (salary['max'] + salary['min']) / 2\n",
    "    sigma = 1000\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as filehandler:\n",
    "        \n",
    "        for id in tqdm(range(1, n_rows + 1), total=n_rows, desc=\"Building dataset...\"):\n",
    "            row = [\n",
    "                f'{id}', \n",
    "                f'{random.choice(firstnames)}', \n",
    "                f'{random.choice(lastnames)}', \n",
    "                f\"{random.randint(age['min'], age['max'])}\",\n",
    "                f\"{round(float(random.normalvariate(mu=mu, sigma=sigma)), 2)}\"\n",
    "            ]\n",
    "            rows.append(row)\n",
    "            if id % chunk_size == 0:\n",
    "                filehandler.writelines([f\"{','.join(row)}\\n\" for row in rows])\n",
    "                rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f1e701f-768a-42df-bf8b-f7a85bd935bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000000/20000000 [01:53<00:00, 176590.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# około 715MB zostanie zapisanych w pliku csv, dostosuj ilość rekordów do swoich potrzeb\n",
    "build_dataset('employee.csv', 20_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c97e0b9-6934-44db-91a1-033f7c1275e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000000/1000000 [00:05<00:00, 173197.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# około 715MB zostanie zapisanych w pliku csv, dostosuj ilość rekordów do swoich potrzeb\n",
    "build_dataset('employee_1m.csv', 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09598a25-6007-4b20-a908-fcbb26a5f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 ms, sys: 669 μs, total: 16.5 ms\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# więcej magicznych metod w Jupyter Notebooku: https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# wczytanie pliku csv przez spark\n",
    "# df = spark.read.csv('employee.csv', header=True)\n",
    "df = spark.read.csv('employee.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8d73f74-3924-4c07-a1a0-1ba6b86b260f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63c2fed8-ea9c-43d7-a338-bb713da77903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+----------+-------------------+---+-------+\n",
      "| id| firstname|           lastname|age| salary|\n",
      "+---+----------+-------------------+---+-------+\n",
      "|  1|   Wisława|             Szczaw| 20|7070.58|\n",
      "|  2|      Adam|             Wlotka| 52|8025.71|\n",
      "|  3|Mieczysław|         Malinowski| 29|7676.51|\n",
      "|  4|Mieczysław|       Mieczykowski| 31|6641.61|\n",
      "|  5|Aleksandra|         Wróblewski| 24|9787.46|\n",
      "|  6|  Wojciech|               Glut| 44|8260.26|\n",
      "|  7|Aleksandra|               Glut| 19|4774.99|\n",
      "|  8|Aleksandra|         Wróblewski| 59|6790.58|\n",
      "|  9|Aleksandra|Brzęczyszczykiewicz| 60|7726.46|\n",
      "| 10|   Wisława|           Barański| 64|7163.95|\n",
      "+---+----------+-------------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 4.01 ms, sys: 0 ns, total: 4.01 ms\n",
      "Wall time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wypisujemy schemat i 10 pierwszych wierszy utworzonego obiektu Spark DataFrame\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b12d13a-8bf2-44b2-994e-10816fb8d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład wykorzystania funkcji transform, która mapuje wykonanie stworzonej funkcji tu_upper_str_columns na istniejącą kolumnę\n",
    "# i zwraca nową ramkę z dodatkową kolumną\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "def to_upper_str_columns(df, column_name, new_column_name):\n",
    "    return df.withColumn(new_column_name, upper(df[column_name]))\n",
    "\n",
    "df = df.transform(to_upper_str_columns, \"firstname\", \"firstname_upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d0629ee-1d04-4060-b1dd-b2c4c5d41903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+---+-------+---------------+\n",
      "| id| firstname|           lastname|age| salary|firstname_upper|\n",
      "+---+----------+-------------------+---+-------+---------------+\n",
      "|  1|   Wisława|             Szczaw| 20|7070.58|        WISŁAWA|\n",
      "|  2|      Adam|             Wlotka| 52|8025.71|           ADAM|\n",
      "|  3|Mieczysław|         Malinowski| 29|7676.51|     MIECZYSŁAW|\n",
      "|  4|Mieczysław|       Mieczykowski| 31|6641.61|     MIECZYSŁAW|\n",
      "|  5|Aleksandra|         Wróblewski| 24|9787.46|     ALEKSANDRA|\n",
      "|  6|  Wojciech|               Glut| 44|8260.26|       WOJCIECH|\n",
      "|  7|Aleksandra|               Glut| 19|4774.99|     ALEKSANDRA|\n",
      "|  8|Aleksandra|         Wróblewski| 59|6790.58|     ALEKSANDRA|\n",
      "|  9|Aleksandra|Brzęczyszczykiewicz| 60|7726.46|     ALEKSANDRA|\n",
      "| 10|   Wisława|           Barański| 64|7163.95|        WISŁAWA|\n",
      "+---+----------+-------------------+---+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c11061ba-f89b-4d10-8121-87259906e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "316480"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtrowanie numeryczne, ale tu na kolumnie typu str - czy jest poprawne?\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fddcbaf-88b6-401d-a994-caf07569c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# na ile partycji została nasza ramka danych rozrzucona po \"klastrze\"?\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d991fec2-9fe2-4e45-8636-01dc0b0274ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.16 ms, sys: 9.03 ms, total: 15.2 ms\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "316480"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# mierzymy czas operacji przy domyślnej liczbie partycji\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba2ecb67-4bae-407a-9495-f4466440e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dad72af7-d04c-498c-8c2e-a40bcd2fd62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36453b78-51c5-417a-9ea8-e46d7bba2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 166 μs, total: 14.1 ms\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "316480"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# mierzymy czas operacji przy 12 partycjach dla 20_000_000 rekordów\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9520db5-8f20-4b7b-9b6b-b4a903ebc199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 17:54:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 65:==============================================>         (10 + 2) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|firstname_age|   18|   19|   20|   21|   22|   23|   24|   25|   26|   27|   28|   29|   30|   31|   32|   33|   34|   35|   36|   37|   38|   39|   40|   41|   42|   43|   44|   45|   46|   47|   48|   49|   50|   51|   52|   53|   54|   55|   56|   57|   58|   59|   60|   61|   62|   63|   64|   65|   66|   67|   68|\n",
      "+-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|         Adam|38925|39000|39342|39215|39107|39216|38883|39065|39092|39073|39553|38934|38949|39096|38920|39389|39398|39031|39314|39147|39418|39017|39266|38907|38986|38868|39314|39110|39041|39401|38913|39418|39053|39061|38981|39458|39375|39052|38981|39214|39425|39394|39180|39342|39151|38984|39048|39154|39166|39334|39031|\n",
      "|        Agata|39299|39085|39238|39444|39449|38978|39058|39372|39253|39331|39113|39262|39384|39330|39462|39115|39427|39552|38946|38991|39095|38877|39161|39282|39345|39138|39291|39120|39020|39340|39278|39283|39131|39158|39111|39348|38969|39243|39144|39504|39128|38915|39078|39171|39193|39281|39189|39129|38991|39708|39130|\n",
      "|   Aleksandra|39166|39194|39566|39248|39446|39356|38987|39447|39194|39252|39157|39526|38995|39263|39247|39407|39280|39162|38985|39594|39221|39570|39172|39189|39339|39243|39210|39450|39123|38910|38734|39435|39154|39031|39381|39149|39334|39221|39119|39394|39394|39417|39056|39420|39420|39155|39623|38929|39280|39224|39225|\n",
      "|    Katarzyna|39040|39067|39191|39183|39168|39162|39292|38887|39587|39210|39466|38982|39346|39216|39586|38942|39287|39288|39234|39238|39348|39112|39165|39356|39409|39183|39459|39528|39030|38972|39170|39327|39039|39132|39397|39147|39131|39250|39320|39171|39335|39075|38939|39120|39252|38906|39106|39407|39368|39091|39041|\n",
      "|    Krzysztof|39229|39131|39010|39267|39628|39078|39438|39016|39428|39009|39062|39415|39028|39152|39349|39493|39208|39502|39621|39215|39421|39236|39264|39191|39438|39277|39418|38833|39403|39336|39453|39182|39492|39007|39084|39031|39159|39454|39435|38952|39054|39363|38984|39305|39259|39189|39184|38977|39130|39355|39764|\n",
      "|        Marek|39220|39321|39268|39236|39415|39367|39467|39506|39310|39107|39042|39195|39012|39473|39333|39417|39131|39403|39139|39286|38972|39172|39408|39069|39363|39189|39366|38939|38854|39142|39108|39167|38937|39740|39510|39208|39051|39247|39374|39073|39221|39281|39317|39150|39153|39392|39130|39296|38931|39415|39080|\n",
      "|   Mieczysław|39100|39231|39036|39125|39231|39083|38992|39344|39086|39158|39087|39268|39607|39312|39071|39500|38931|39289|38845|39333|39107|39277|39143|39655|38931|39052|39440|39031|39020|39255|39085|38977|39206|39271|39193|39091|39061|39312|39180|38913|39148|38894|38871|39572|39444|39272|39235|39229|39174|39561|39054|\n",
      "|      Wisława|39215|38803|39437|39470|39096|39213|39014|39418|39049|39088|39194|39364|39165|39083|39339|39373|39409|39047|39335|38731|38869|38877|39358|39170|38991|39183|39376|39137|39454|39074|39000|39162|39525|39182|39481|39193|39235|39242|39334|39121|38771|38954|39661|39367|39103|39385|39011|39289|39264|39313|39535|\n",
      "|     Wojciech|39220|39229|38999|39005|39300|38911|39353|39404|39025|38946|39402|39074|39129|39377|39399|39350|39391|39196|39188|39475|39566|39276|38980|39032|39100|39139|39269|39444|39001|39552|39114|39011|39241|39154|39135|39315|39299|39210|39214|38673|39178|39294|39225|39614|39319|39335|39440|39014|38976|39257|39177|\n",
      "|     Zbigniew|39196|39054|39445|39190|39057|39408|39125|39271|39736|39539|39177|39043|39185|39077|39425|39322|39311|38955|38913|39253|39410|39084|39381|39039|39091|39297|39624|39178|39217|39175|39254|39557|39123|39250|39240|39270|39229|39282|39066|39495|39292|39123|38804|39102|39316|39184|39226|39364|39413|39347|39219|\n",
      "+-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# macierz częstości dla dwóch kolumn - uwaga dla bardzo różnorodnych danych!\n",
    "df.crosstab(\"firstname\", \"age\").sort(\"firstname_age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d400445-c5a8-4095-bf9f-833f49a2c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- Exchange (4)\n",
      "   +- Project (3)\n",
      "      +- Filter (2)\n",
      "         +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [5]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/spark/work-dir/Lab6/employee.csv]\n",
      "PushedFilters: [IsNotNull(firstname), StringContains(firstname,ski)]\n",
      "ReadSchema: struct<id:int,firstname:string,lastname:string,age:int,salary:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [5]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059]\n",
      "Condition : (isnotnull(firstname#1056) AND Contains(firstname#1056, ski))\n",
      "\n",
      "(3) Project\n",
      "Output [6]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059, upper(firstname#1056) AS firstname_upper#1092]\n",
      "Input [5]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059]\n",
      "\n",
      "(4) Exchange\n",
      "Input [6]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059, firstname_upper#1092]\n",
      "Arguments: RoundRobinPartitioning(12), REPARTITION_BY_NUM, [plan_id=1068]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [6]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059, firstname_upper#1092]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# funkcja explain może przydać się w przypadku bardziej zaawansowanego debuggingu, optymalizacji i zrozumienia\n",
    "# kolejności działania niektórych elementów silnika Spark\n",
    "query = df.filter(df.firstname.contains('ski'))\n",
    "query.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fc1421b-e04d-4a35-93be-2f48251ca20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# zapisujemy ramkę do plików parquet\n",
    "# zwróć uwagę na liczbę utworzonych plików\n",
    "\n",
    "df.write.mode('overwrite').parquet('./data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "703ab6e9-cdd4-42f4-bbf4-c323f94571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
